00:00:01	so these are topics for today basically so what are hdfs blocks so again this we covered this yesterday also but today we are going to discuss in detail so whenever a file comes up a file is always make up made up of blocks when you talk of a Unix file system the block size is really small but when it comes to Hado the file system would be a big file and the file would be uh split into multiple blocks based on the Block size of the cluster so if you define your block size as 64 MB then any file that

00:00:31	comes to a data node all these blocks will be 64 MB blocks so if this file was say um 128 MB file or 256 MB file so that means it will have uh four blocks so this file will be split into four blocks and based on replication there will be four * three blocks so there will be 12 basically blocks for a four like block file based on the replication Factor if replication factor is one you will just have four blocks if replication factor is three you will have three different as you as you can see there's a blue colored blocks there

00:01:08	are orange color blocks and there are green color blocks so these basically signify the replication and these blocks then end up on data notes based on where name node tries decides to put the blocks and the name node basically is now managing the complete file system as we discussed so there's a file system image which is made up of the blocks in the system all the data nodes basically tell their blocks whenever a data node comes up it will tell its blocks to name node that I have these these these these

00:01:37	blocks so all the blocks for example in data node one you can see this blue red green colored blocks so these are the blocks that are on data node one similar data node 2 has blocks so whenever data node comes up it will tell that I have these five blocks so name node will basically make a note that these these these these blocks are on this data node and then it will have file system image where it will basically know the file and it will have the blocks and from data nodes it will get the block report

00:02:02	so now it will know that which blocks are on which data node and then that basically that file system image plus this inmemory metadata from data nodes which tells the block mapping to the data node comprise tells that name note clearly that where actually a file would be and and then we have something called edit logs so whenever some change is done on the file system a file gets removed a file gets created then we have edit logs like you have bin log for example in my SQL so so based on that we

00:02:32	have these transactional logs which are called edit logs so so basically if you want to have the complete file system image at any point of time what you'll have to do is you have to look at the initial FS image which is which is the file system image that's when name note came up and then after that changes all the changes that happen to the file system they go under edit log so if you basically combine file system image and all the edit logs that that are present in the name node metata directory if you

00:03:00	combine them together those together combined will tell you the file system State at at the point of time which is now so basically that's how the file system image is built so again we going to discuss this in detail but this is basically just telling you that what are basically sgfs blocks where they are stored and how name node knows where the blocks are stored so Sanjay has a question when a data node goes down then the block info is removed from the fs image so FS image basically is the image

00:03:31	that came up yes so so as I said there's inmemory file system metadata so where whatever blocks are there on a data node they are in name node's memory as a bit map so if a data node goes down the name node will not receive any hardbeat and then it will basically update those c those those blocks uh for the data node as stale and it will basically try to base on your replication factor for example it was three and then say one data node went down so all these block blocks the first Blue Block for example

00:04:01	what is now replicated on two nodes only this and this the name node will start replicating this block to another node and once it copies that block to another data node then it will update that information in his bitmap that now the new data node third data node for this block is the new node where it just replicated it so name node basically takes care of re-replication of the blocks in case a data node goes down so hopefully that answer your question Sanjay and S is saying is the default to 64 MB each blocks so yes so

00:04:35	default by default if you don't specify block size on Hado by default it is 64 MB on all the blocks on the system so basically all data noes will store 64 MB blocks so fan is saying can you come again what exactly FS image contains and edit log contains what is the default file size of f image and edog when we set up the name node yeah so default files system size would be like very tiny because because initially you don't have any anything on your file system M so again once I show the demo today you

00:05:05	will see that initially file system image would be like few bytes because there would nothing in file system image so as and when you start adding nodes the file system image will be built up and then and then we discussed yesterday that secondary name note does some checkpointing so when I said that at any point of time if you want to see the state of your file system image you basically combine initial file system image with the edit logs so this combination this merging of file system image with adding logs is done by

00:05:33	secondary name node so again in our further sessions we'll see how secondary name node does it so basically that's why I said that FS image is like constantly if your secondary name node is running it will constantly update that file system image by merging the edit logs at a point of time and then those file system image will actually if your secondy name node is running your file system image size will keep on changing every hour because what secondary name node will do is we will take the original file system image take

00:06:03	all the edit logs that happen in one hour one hour merge them together and send the new file system image to name node and then name node will start using that file system image and all the edit logs that happen after are written in the new edit log so every hour secondary name note does this thing it will merges file system image with the edit logs and give it back to the name node so that's why at any point of time if you want to see real file system State you have to use the file system image present on the

00:06:31	name node plus the edit logs because edit logs have all the deletes the new uh mkdirs all the new U directory Creations also all kind of transactions they go in edit logs so hopefully that answers your question Fanny and so Fanny saying so it's like the data file and log file in SQL Server yes so it's like pin logs right so you have the transactional logs so it's it's like that so basically FS image is like file system image that ini ially comes up so Sanjay is saying when is edit log sync with FS so sanj as I just explained

00:07:07	this to find so I hopefully that answers your question that every hour secondary name node is going to take file system image and take edit logs at a particular point of time or a transaction ID and it marks that transaction ID that I have taken edit log still this transaction ID so edit log basically has a transaction ID so so secondary name not will combine file Sy some image and edit logs and send it back to name node and then name node will use this new file system image and and then all the edit logs starting

00:07:38	from the transaction ID till that the point where secondary name node has basically merged so next time secondary name node will start looking at the edit logs from that new transaction ID that it saved at last hour so that's how this merging happens and again we're going to cover it detail when we discuss secondary name node and then we'll look at the hdf architecture we also briefly discussed this yesterday so we are saying that client is a machine from where you basically you are running Hadoop

00:08:06	commands file system commands to name node so all the metadata operations to read data write data happen from client and client contacts name node name node is storing the metadata which means all the files and knows how what is the replication of each file and what are the blocks made up made up of each uh for each file and knows which data noes have those blocks and data noes are basically storing the blocks and replication happens across the data nodes based on the replication Factor you have defined and then you have

00:08:35	multiple racks so you might have one rack having few data nodes second rack having few data nodes so again rack awareness is something we are going to discuss in today's class so for your questions uh rack awareness I'm going to discuss again and then so whenever client needs to read basically it goes to name node and it basically gets the location of data nodes and read the nodes the blocks from data noes directly without talking to the name node similarly when a client has to write it basically talks to name node gets the

00:09:07	location and starts writing the blocks with the data noes directly without contacting the name node so name node just tells them this is this is the nodes where you should write the data this is the node you should read from the data and then name noes part is over because otherwise name node will be a bottleneck for everything if everything goes through name node so that's why name node just tells the location of blogs and then clients they talk directly to the data notes to read or write the data okay so Muhammad is saying can we

00:09:35	change time of syncing yes so by default it is 1 hour but you can definitely change it yes so Hado is very configurable in terms of allowing you to change the parameters so this is how basically hdfs blocks look like so I explained the fs image I said I and then I explained about edit logs and then I explained about a bitmap map right so what I said was name node has a list of files and name node has list of blocks per file and name node has list of data notes per block so that's the bitmap that stays in memory so list of

00:10:13	files is basically file system image and the list of blocks per file is also in file system image edit logs are the real edits happening to the file system which means deltion of a directory edition of a directory so those are edit logs and other than that what are the number of list of data nodes per block so every block based on replication Factor will have certain number of certain data nodes where the data would be written right so one block has three data nodes because that block based on replication

00:10:42	Factor there'll be three data nodes that will basically having the copy of that block so this is basically the memory part of metadata that stays in name node memory that what is the list of data nodes per block so that's basically comprising the name note metadata [Music] so money is saying when name node is recovering from a data node crash will there be any impact to the availability no because because we have a replication Factor right so even if once a data node went down name node will still have in

00:11:13	memory that this block has these two data noes where it is available so whenever a request comes to read that block name node will not use the data node that has gone down because it has already updated its bit map and it will just go to the next available data node and that data node will have the block client will read directly from that block from that data node so if a data node is down it does not impact clusters availability and people can still read and write the files so that's basically the main beauty of Hado that

00:11:43	even if your few nodes are down your cluster should still be able to operate and people should still be able to read and write the data without wa waiting for that Noe to come up so sanj is saying how does name node decide which data I have to pick pick from three replicated nodes so basically name node will just basically uh has the algorithm that it will write that this block has these three data nodes so if you have no rack awareness it could be in any order so when we have rack awareness then name note decides in a

00:12:12	particular order that we'll discuss in in the coming few slides so sanj that would answer your question that how name node will decide which data node to pick to write and to tell the client so while has a question if a task tracker is running on a data node that went down what happens to the task tracker so basically if so if if we say that data node process went down that means just the jvm for data node went down that means the the host where data node and task track were running then that host is still up and data node went

00:12:46	down for some reason then the task tracker would not be impacted right because data node is a separate Java process and task tracker is a different jvm so if data node went down it doesn't impact task tracker but if say your host went out of memory in that case the host is just not up so in that case both data node as well as Tas tracker both of them would be down so hopefully that answers your question well Su things what is what process is writing the data block so I guess sum in our next slide will answer that question









00:00:08	introduction to map reduce mapreduce is a programming model that processes and analyzes huge data sets logically into separate clusters while map sorts the data reduce segregates the data into logical clusters thus removing the bad data and retaining necessary information why mapreduce prior to 2004 huge amounts of data were stored in single servers if any program ran a query for the data stored in multiple servers logical integration of the search results and analysis of the data was a nightmare

00:00:48	not to mention the massive efforts and expenses that were involved the threat of data loss challenge of data backup and reduced scalability resulted in the issue snowballing into a crisis of sorts to counter this google introduced mapreduce in december of 2004 and the analysis of datasets was done in less than 10 minutes rather than 8 to 10 days queries could run simultaneously on multiple servers and search results could be logically integrated and data could be analyzed in real time the usps of mapreduce are its fault

00:01:29	tolerance and scalability let's look at a mapreduce analogy the mapreduce steps steps present here daniel's vote counting after an election as an analogy in step one each polling booth ballot papers are counted by a teller this is a pre-map reduce step called input splitting in step two tellers of all boos count the ballot papers in parallel as multiple tellers are working on a single job the execution time will be faster this is called the map method in step 3 the ballot count of each booth under the assembly and parliament seat

00:02:12	positions is found and the total count for the candidates is generated this is known as the reduce method thus map and reduce help to execute the job quicker than an individual counter the mapreduce analogy vote counting is an example to understand the use of mapreduce the key reason to perform mapping and then reducing is to speed up the job execution of a specific process this can be done by splitting a process into a number of tasks thus enabling parallelism if one person counts all of the ballot

00:02:50	papers and waits for others to finish the ballot count it could take a month to receive the election results when many people count the ballot papers simultaneously the results are obtained in one or two days this is how mapreduce works let's look at a word count example in this screen the mapreduce operation is explained using a real-time problem the job is to perform a word count of the given paragraph on the left the sentence in input says the quick brown fox jumps over a lazy dog and a dog is a man's best friend we will

00:03:30	then take this sentence through the corresponding steps of splitting mapping shuffling and reducing the mapreduce process then begins with the input phase which refers to providing data for which the mapreduce process is to be performed the sentence used is as input here the next step is the splitting phase which refers to converting a job submitted by the client into a number of tasks in this example the job is to split into two tasks one for each sentence then the mapping phase refers to generating a key

00:04:08	value pair for the input since this example is about counting words the sentence is now split into words by using the sub string method to generate words from lines the mapping phase will ensure that the words generated are each converted into keys and a default value of 1 is allotted to each key or each word in the sentence in the next step the shuffling phase refers to sorting the data based on those keys as shown on screen the word sorted into ascending order the last phase is the reducing phase

00:04:48	in this phase the data is reduced based on the repeated keys by incrementing the value of each key where there's a duplicate word the word dog and letter a are repeated therefore the reducer will delete the key and increase the value depending on the number of occurrences of the key this is how the mapreduce operation is performed map execution phases map execution consists of five phases the mapping phase the partition phase the shuffle phase the sort phase and the reduce phase the assigned input split is read from

00:05:32	hdfs where split could be a file block by default furthermore input is parsed into records as key value pairs the map function is applied to each record to return zero or more new records these intermediate outputs are stored in the local file system as a file they are sorted first by bucket number and then by a key at the end of the map phase information is sent to the master node after its completion in the partition phase each mapper must determine which reducer will receive each output for any key regardless of which map or

00:06:17	instance generated it the destination partition is the same so for a single word that word would always go to the same destination partition note that the number of partitions will be equal to the number of reducers in the shuffle phase input data is fetched from all map tasks for the portion corresponding to the reduced task's bucket in the sort phase a merge sort of all map outputs occurs in a single run and finally in the reduce phase a user defined reduce function is applied to the merged run

00:06:54	the arguments are a key and the corresponding list of values the output is written to a file in hdfs map execution in a distributed two node environment the mappers on each of the nodes are assigned to each input split a box based on the input format the record reader reads the split as a key value pair the map function is applied to each record to then return zero or more new records these intermediate outputs are stored in the local file system thereafter a partitioner assigns the records to the reducer

00:07:35	in the shuffling phase the intermediate key value pairs are exchanged by all nodes the key value pairs are then sorted by applying the key and reduce function again the output is stored in hdfs based on the specified output file format the essentials of each mapreduce phase are shown on the screen the job input is specified in key value pairs each job consists of two stages first a user defined map function is applied to each input record to produce a list of intermediate key value pairs second a user defined reduce function is

00:08:16	called once for each distinct key in the map output then the list of intermediate values associated with that key is passed the essentials of each mapreduce phase are as follows first the number of reduced tasks can be defined by the users second each reduced task is assigned a set or record groups that is intermediate records corresponding to a group of keys third for each group a user defined reduce function is applied to the recorded values and four the reduced tasks are read from every map task and

00:08:57	each read returns the record groups for that reduced task reduce phase cannot start until all mappers have finished processing so combining your output is an important step once all the tasks are completed mapreduce job a job is a mapreduce program that causes multiple map and reduced functions to run parallelly over the life of the program many copies of map and many copies of reduce functions are forked for parallel processing across the input dataset a task is a map or reduce function executed on a subset of this data

00:09:43	with this understanding of job and task the application master and node manager functions become easier to comprehend first the application master is responsible for the execution of a single application or mapreduce job it divides the job requests into tasks and assigns those tasks to node managers running on one or more slave nodes the node manager has a number of dynamically created resource containers the size of a container depends on the amount of resources it contains such as memory cpu disk and network io

00:10:24	it executes map and reduced tasks by launching these containers when instructed to by the mapreduce application master mapreduce and associated tasks the map process is an initial step to process individual input records in parallel the reduced process is all about summating the output with a defined goal as coded in the business logic the node manager keeps track of individual map tasks and can run in parallel a map job runs as part of a container execution by node manager on a particular data node within a cluster

00:11:03	the application master keeps track of a map reduced job the hadoop map produce job work interaction initially a hadoop mapreduce job is submitted by a client in the form of an input file or a number of input splits of files each containing data the mapreduce application master will then distribute the input split to separate node managers the mapreduce application master then coordinates with those node managers the mapreduce application master will now resubmit the task to an alternate node manager if that data node

00:11:42	should fail the resource manager gathers the final output and informs the client of success or failure status let's look at the characteristics of mapreduce mapreduce is designed to handle very large scale data in the range of petabytes and exabytes it works well on write once and read many data sets also known as worm data mapreduce allows parallelism without mutexes the map and reduce operations are performed by the same processor those operations are provisioned near the data as data locality is preferred

00:12:24	in other words we will move the application to the data and not the other way around commodity hardware and storage is leveraged in mapreduce to keep things cost effective and the runtime takes care of splitting and moving data for operations some of the real-time uses of mapreduce are as follows simple algorithms such as grep text indesign and reverse indexing such things as data intensive computing which would include sorting large and small sets of data stream data and structured data data mining operations such as bays in

00:13:01	classification which you'll study later and search engine operations such as keyword indic ad rendering and page ranking enterprise analytic analytics to ensure the business is operating smoothly and with the best decision making data available gaussian analysis for locating extraterrestrial objects in astronomy which uses very large data sets and semantic web and web 3.0 indicing and operations data types in hadoop data types in hadoop the first data type is text the function of this data type is stored

00:13:42	to string data the writable data type stores integer data long writable as the name suggests stores long data similarly other data types are float writable for storing float data and double writable for storing double data there is also boolean writable and byte-writable data types null writable is a placeholder when a value is not needed this illustration here shows a sample data type that you can create on your own this data type will need you to implement a writable interface as you can see writable will define a

00:14:22	deserialization or serialization protocol every data type in hadoop is a writable writable comparable will define your sort order all keys must be of this type but not value then in writable and long rideable and the various concrete classes that you'll define for your different data types lastly sequence files refers to a binary encoded with a sequence of key value pairs input format it's in mapreduce mapreduce can specify how its input is to be read by defining an input format the table lists some of the classes of

00:15:04	input formats provided by the hadoop framework let's look at each of them the first class is key value text input format which is used to create a single key value pair per line text input format is used to create a program that considers a key as the line number and a value as the line itself n-line input format is similar to text input format except that there are n number of lines that make an input split multi-file input format is used to implement an input format that aggregates sequence style one format to be

00:15:44	implemented the input file must be a hadoop sequence file which gains serialized key value pairs we set the environment for mapreduce development first let's ensure that all hadoop services are live and running this can be verified in two steps first use the command jps as shown type sudo jps and then look for all five services that you need node data node node manager resource manager and secondary name node there may be additional services that are used by a hadoop cluster but these are the ones that we require as core

00:16:28	services next let's look at uploading big data and small data the command to upload any data big or small from the local file system to hdfs is hadoop space fs space dash copy from local space and then the source file address space and the destination file address now let's look at the steps of building a mapreduce program first determine if the data can be made parallel and solved by using mapreduce for example you need to analyze whether the data is write once read many or worm type data in nature then design and

00:17:11	implement a solution as a mapper and then reducer class within your code compile the source code with hadoop core and package the code as a jar executable configure the application job as the number of mapper and reducer tasks and to the number of input and output streams then load the data or use it on previously available data and then launch and monitor the job you can then study the results and repeat any of the previous steps as needed the hadoop mapreduce requirements the user or developer is required to set

00:17:51	up the framework with the following parameters the locations of the job input in the distributed file system the location of the job output in the distributed file system the input format to use the output format to use define a class containing the map function and then a separate class containing the reduce function which is optional if a job does not need a reduced function there is no need to specify a reducer class in your code the framework will partition the input schedule and execute map tasks across

00:18:32	the cluster if requested it will sort the results of the map task and it will execute the reduced tasks with the map output the final output will be moved to the output directory and the job status then reported to the user set of classes this image shows the set of classes under the user supply and the framework supply the user supply reversed to the set of java classes and the methods provided to a java developer for developing hadoop mapreduce applications the framework supply refers to defining

00:19:10	the workflow of a job which is followed by all hadoop services as shown in the image the user provides the input location and the input format as required by the program logic once the resource manager accepts the input a specific job is divided into tasks by the application master each task is then assigned to an individual node manager once the assignment is complete the node manager will start the map task it performs shuffling partitioning and sorting for individual map outputs once the sorting is complete the reducer

00:19:52	starts the merging process this is also called the reduce task the final step is collecting the output which is performed once across all the individual tasks once they're completed this reduction is based on programming logic let's look at mapreduce responsibilities the basic user or developer responsibilities of mapreduce are one setting up the job two specifying the input location and three ensuring that the input is in the expected format and location the framework responsibilities of mapreduce are distributing jobs among

00:20:33	the application master and node manager nodes of the cluster running the map operation then performing the shuffling and sorting operations next are the optional reducing phases and finally placing the output in the output directory and informing the user of the job completion status we create a new project let's see how we would create a new project first make sure eclipse is installed on your system once it's installed you can create a new project and add the essential jar files to run a mapreduce program

00:21:16	then to create a new project click the file menu select new project or alternatively press control n to start the wizard of a new eclipse project the screen shows the new project and select wizard options for the first step in step number two you would select a java project from the list then click the next button to continue step 3 the newly created project has to have a name in this case we'll type the project name as word count and click the next button to continue in step number four of your new project

00:22:02	you will now include jar files from the hadoop framework to ensure that the programs locate the dependencies to one location and in step number five you will add the essential jar files to locate these go to the libraries tab and click the add external jars button to add the essential jar files after adding the jar files click the finish button to complete the project successfully next we'll check the hadoop environment to ensure we have mapreduce it is important to check whether the machine setup can perform mapreduce

00:22:41	operations to verify this use the example jar files that are deployed by a hadoop installation this can be run by running the command shown on the stream before executing this command ensure that the words.txt file resides in the data first location in the onscreen example you see the hadoop jar command being executed passing it the hadoop examples dot jar file set word count and then data first slash words dot txt as the input and data first slash output as the selected output advanced mapreduce

00:23:25	hadoop mapreduce uses data types to work with user given mappers and user given reducers the data is read from files into the mapper and emitted by mappers to the reducers the processed data is sent back by the reducers data emitted by reducers goes into output files at every step data is stored in java objects let's now understand the writable data types in advanced mapreduce in the hadoop environment all input and output objects across the network must obey the writable interface which allows

00:24:05	hadoop to read and write data in a serialized form for transmission let's look at hadoop interfaces in some more detail the interfaces in hadoop are writable and writable comparable as you've already seen a writable interface allows hadoop to read and write data in a serialized form for transmission a writable interface consists of two methods read and write fields a writable comparable interface extends the writable interface so that the data can be used as a key and not as a value as shown here the writable comparable

00:24:48	implements two methods compare to and hash code output formats in mapreduce now that you completed the input formats in mapreduce let's look into the classes for the mapreduce output format the first class is default output format which is text output format it writes records as lines of text each key value pair is separated by a tab character this can be customized by using the mapreduce text output format dot separator property the corresponding input format is key value text input format sequence file output format writes

00:25:30	sequence files to save on output space this represents a very compact and compressed version of normal data blocks sequence file as binary output format is responsible for writing key value pairs that are in raw binary format into a sequential file container and map file output format writes map files as the output the keys in a map file are added in a specific order the reducer then emits keys in that sorted order multiple text output format writes data to multiple files whose names are derived from the output keys and

00:26:10	multiple sequence file output format creates output in multiple files in a compressed form let's look at distributed caching a distributed cache is a hadoop feature to cache files that are needed by the applications a distributed cache will help boost efficiency when a map or reduced task needs access to common data it allows a cluster node to read the imported files from its local file system instead of retrieving the files from other cluster nodes in the environment it allows both single files and archives

00:26:46	such as zip and car.gz it copies files only to slave nodes if there are no slave nodes in the cluster then distributed cache copies the files to the master node it allows access to the cached files from mapper or reducer applications to make sure that the current working directory is added into the application path and allows referencing of the cache files as though they were present in the current working directory vastly speeding up access using distributed cache step one first set up the cache by copying the

00:27:23	requisite files to the file system as shown here here we see a bin hadoop fs command using a dash copy from local of the file lookup dot dat to hcfs myout lookup.dat this shows us that there is currently no file or directory of that name the hadoop fs copy from local remember will take a file from your local file system and place it in the target directory within the hdfs file system using distributed cache step 2 set the application's job conf as shown in the example in this case we're setting up a new

00:28:07	instance of job by creating a new instance of job conf we then will call a distributed cache add cache file method and create a new uri specifying the location in this case my app lookup.dat with the filename of lookup.dat in the same way each of the different commands here shows you creating a cache entry for zip files jar files tar files tgz and gz files in step three of setting up your distributed cache you will use the cache files in the mapper or reducer class that you create once the private path and configure

00:28:49	information is in your program you simply declare an instance of file called f specifying a new file along with the parameter of dot map dot zip slash sum file in zip dot text this will map your file into the distributed cache it joins in mapreduce joins are relational constructs that can be used to combine relations in mapreduce joins are applicable in situations where you have two or more data sets you want to combine a join is performed either in the map phase or later on in the reduce phase by

00:29:28	taking advantage of the map reduce sort merge architecture the various join patterns available in map produce are reduced side join replicated join composite join and cartesian product a reduced side join is used for joining two or more large data sets with the same foreign key with any kind of join operation a replicated join is a map side join that works in situations where one of these data sets is small enough to cache that's vastly improving its performance a composite join is a map side join used

00:30:08	on very large formatted input data sets sorted and partitioned by a foreign key and lastly a cartesian product is a map side join where every single record paired up with another full data set this style of join typically takes a significantly longer period of time to execute a reduced side join works in the following ways the mapper first prepares for join operations it takes each input record from every data set and emits a foreign key record pair the reducer then performs a join operation where it collects the values

00:30:49	of each input group into temporary lists the temporary lists are then iterated over and the records from both sets are now joined a reduce side join should be used in the following conditions when multiple large data sets are being joined by a foreign key or when flexibility is needed to execute any join operation or when a large amount of network bandwidth is available as will be moving data across the network and also when there is no limitation on the size of data sets the sql analogy of a reduced side join

00:31:30	is given on the screen in the output of a reduced side join the number of part files equals the number of reduced tasks so if you have 10 reduced tasks you will have 10 separate part files replicated joins a replicated join is a map only pattern in other words does not use the reduce phase and works as follows it reads all files from the distributed cache and then stores them in in memory lookup tables the mapper processes each record and joins it with the data stored in memory there is no data shuffled to the reduced

00:32:08	phase the mapper gives the final output part this type of join is typically very quick replicated joins should be used when all data sets except for the largest one can fit into the main memory of each map task that is limited by the size of your java virtual machine or jvm heap size when there is a need for an inner join or a left outer join with the large input data set being the left part of the operation a sql analogy of this type replicated join is given on the screen in the output of a replicated join the

00:32:45	number of part files equal the number of map tasks and again as this is using memory as one side of the join it is typically much faster a composite join is a map only pattern working in the following ways all data sets are divided into the same number of partitions each partition of dataset is sorted by a foreign key and all the foreign keys reside in the associated partition of each data set two values are retrieved from the input tuple associated with each data set based on the foreign key and the output

00:33:23	to the file system this type of join is typically very lengthy and depending on the size of your data sets can run for a very long time the composite join should be used when all data sets are sufficiently large and when there is a need for an inner join or a full outer join a sql analogy of a composite join is displayed on the screen in the output of a composite join the number of part files equal the number of mapped tasks the cartesian product a cantigia product is a map only pattern that works in the following ways

00:34:00	data sets are split into multiple partitions each partition is fed to one or more mappers for example in the image shown here split a-1 and split a-2 are fed to three mappers each a record reader will read every record of input splits associated with the mapper and the mapper simply pairs every record of a data set with every record of all other data sets the cartesian product should be used when there is a need to analyze relationships between all pairs of individual records and when there are no

00:34:39	constraints on the execution time as these can take a long time in the output of a cartesian product every possible tuple combination from the input records is represented hi there if you like this video subscribe to the simply learn youtube channel and click here to watch similar videos turn it up and get certified click here

